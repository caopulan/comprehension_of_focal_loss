{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(\n",
    "    inputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    alpha: float = -1,\n",
    "    gamma: float = 2,\n",
    "    reduction: str = \"none\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples. Default = -1 (no weighting).\n",
    "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
    "               balance easy vs hard examples.\n",
    "        reduction: 'none' | 'mean' | 'sum'\n",
    "                 'none': No reduction will be applied to the output.\n",
    "                 'mean': The output will be averaged.\n",
    "                 'sum': The output will be summed.\n",
    "    Returns:\n",
    "        Loss tensor with the reduction option applied.\n",
    "    \"\"\"\n",
    "    p = torch.sigmoid(inputs)\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = p * targets + (1 - p) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随便初始化一个输入\n",
    "inputs = torch.tensor([-2, 2, 1.5]) # inputs是网络输出，没有直接的概率意义\n",
    "targets = torch.tensor([0.0, 1.0, 0.0]) # 每一分量表示是该类别的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0018, 0.0018, 1.1373])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_focal_loss(inputs, targets) # 直接用facebook写好的进行调用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面我们分别输出一下每一步的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1192, 0.8808, 0.8176])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用sigmoid而不是softmax来获取是每一类的概率\n",
    "p = torch.sigmoid(inputs)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1269, 0.1269, 1.7014])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用二分类交叉熵来对每一个类别获取交叉熵损失\n",
    "# 这里输出是一个长度3的向量，直接用多分类交叉熵这里会是一个值\n",
    "ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8808, 0.8808, 0.1824])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p_t表示模型分类正确的概率\n",
    "p_t = p * targets + (1 - p) * (1 - targets)\n",
    "p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0018, 0.0018, 1.1373])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取最优的gamma，根据公式得到loss\n",
    "gamma = 2\n",
    "loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2500, 0.7500, 0.2500])\n",
      "tensor([0.0005, 0.0014, 0.2843])\n"
     ]
    }
   ],
   "source": [
    "# 最优的alpha是0.25，这里根据alpha最初的设定来取值（>0.5来加强正类样本权重）\n",
    "alpha = 0.75\n",
    "alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "loss = alpha_t * loss\n",
    "print(alpha_t)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
